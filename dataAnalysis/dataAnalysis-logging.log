2019-01-31 10:14:08 INFO  RatingsCounter$:13 - Creating spark context
2019-01-31 10:14:08 WARN  Utils:66 - Your hostname, senthil resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)
2019-01-31 10:14:08 WARN  Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address
2019-01-31 10:14:08 INFO  SparkContext:54 - Running Spark version 2.3.1
2019-01-31 10:14:09 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-01-31 10:14:09 INFO  SparkContext:54 - Submitted application: RatingsCounter
2019-01-31 10:14:09 INFO  SecurityManager:54 - Changing view acls to: senthil
2019-01-31 10:14:09 INFO  SecurityManager:54 - Changing modify acls to: senthil
2019-01-31 10:14:09 INFO  SecurityManager:54 - Changing view acls groups to: 
2019-01-31 10:14:09 INFO  SecurityManager:54 - Changing modify acls groups to: 
2019-01-31 10:14:09 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(senthil); groups with view permissions: Set(); users  with modify permissions: Set(senthil); groups with modify permissions: Set()
2019-01-31 10:14:10 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 42265.
2019-01-31 10:14:10 INFO  SparkEnv:54 - Registering MapOutputTracker
2019-01-31 10:14:10 INFO  SparkEnv:54 - Registering BlockManagerMaster
2019-01-31 10:14:10 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-01-31 10:14:10 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up
2019-01-31 10:14:10 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-3955c7c0-70fb-40c1-847a-c1da0a92c4d5
2019-01-31 10:14:10 INFO  MemoryStore:54 - MemoryStore started with capacity 884.7 MB
2019-01-31 10:14:10 INFO  SparkEnv:54 - Registering OutputCommitCoordinator
2019-01-31 10:14:10 INFO  log:192 - Logging initialized @3023ms
2019-01-31 10:14:10 INFO  Server:346 - jetty-9.3.z-SNAPSHOT
2019-01-31 10:14:10 INFO  Server:414 - Started @3137ms
2019-01-31 10:14:10 INFO  AbstractConnector:278 - Started ServerConnector@433f9bf9{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-01-31 10:14:10 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.
2019-01-31 10:14:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7db82169{/jobs,null,AVAILABLE,@Spark}
2019-01-31 10:14:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@631e06ab{/jobs/json,null,AVAILABLE,@Spark}
2019-01-31 10:14:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2a3591c5{/jobs/job,null,AVAILABLE,@Spark}
2019-01-31 10:14:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@346a361{/jobs/job/json,null,AVAILABLE,@Spark}
2019-01-31 10:14:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@107ed6fc{/stages,null,AVAILABLE,@Spark}
2019-01-31 10:14:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1643d68f{/stages/json,null,AVAILABLE,@Spark}
2019-01-31 10:14:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@186978a6{/stages/stage,null,AVAILABLE,@Spark}
2019-01-31 10:14:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4052274f{/stages/stage/json,null,AVAILABLE,@Spark}
2019-01-31 10:14:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@132ddbab{/stages/pool,null,AVAILABLE,@Spark}
2019-01-31 10:14:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@297ea53a{/stages/pool/json,null,AVAILABLE,@Spark}
2019-01-31 10:14:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@acb0951{/storage,null,AVAILABLE,@Spark}
2019-01-31 10:14:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5bf22f18{/storage/json,null,AVAILABLE,@Spark}
2019-01-31 10:14:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@267f474e{/storage/rdd,null,AVAILABLE,@Spark}
2019-01-31 10:14:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7a7471ce{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-01-31 10:14:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@28276e50{/environment,null,AVAILABLE,@Spark}
2019-01-31 10:14:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@62e70ea3{/environment/json,null,AVAILABLE,@Spark}
2019-01-31 10:14:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3efe7086{/executors,null,AVAILABLE,@Spark}
2019-01-31 10:14:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@675d8c96{/executors/json,null,AVAILABLE,@Spark}
2019-01-31 10:14:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@741b3bc3{/executors/threadDump,null,AVAILABLE,@Spark}
2019-01-31 10:14:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2ed3b1f5{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-01-31 10:14:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@63648ee9{/static,null,AVAILABLE,@Spark}
2019-01-31 10:14:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@569bf9eb{/,null,AVAILABLE,@Spark}
2019-01-31 10:14:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@61526469{/api,null,AVAILABLE,@Spark}
2019-01-31 10:14:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7c351808{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-01-31 10:14:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@180e6ac4{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-01-31 10:14:10 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://10.0.2.15:4040
2019-01-31 10:14:10 INFO  Executor:54 - Starting executor ID driver on host localhost
2019-01-31 10:14:10 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38845.
2019-01-31 10:14:10 INFO  NettyBlockTransferService:54 - Server created on 10.0.2.15:38845
2019-01-31 10:14:10 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-01-31 10:14:11 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, 10.0.2.15, 38845, None)
2019-01-31 10:14:11 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 10.0.2.15:38845 with 884.7 MB RAM, BlockManagerId(driver, 10.0.2.15, 38845, None)
2019-01-31 10:14:11 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, 10.0.2.15, 38845, None)
2019-01-31 10:14:11 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 38845, None)
2019-01-31 10:14:11 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@64a1923a{/metrics/json,null,AVAILABLE,@Spark}
2019-01-31 10:14:11 INFO  RatingsCounter$:18 - read file...
2019-01-31 10:14:11 INFO  MemoryStore:54 - Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 884.5 MB)
2019-01-31 10:14:11 INFO  MemoryStore:54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 884.5 MB)
2019-01-31 10:14:11 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on 10.0.2.15:38845 (size: 20.4 KB, free: 884.7 MB)
2019-01-31 10:14:11 INFO  SparkContext:54 - Created broadcast 0 from textFile at RatingsCounter.scala:19
2019-01-31 10:14:12 INFO  SparkContext:54 - Invoking stop() from shutdown hook
2019-01-31 10:14:12 INFO  AbstractConnector:318 - Stopped Spark@433f9bf9{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-01-31 10:14:12 INFO  SparkUI:54 - Stopped Spark web UI at http://10.0.2.15:4040
2019-01-31 10:14:12 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!
2019-01-31 10:14:12 INFO  MemoryStore:54 - MemoryStore cleared
2019-01-31 10:14:12 INFO  BlockManager:54 - BlockManager stopped
2019-01-31 10:14:12 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped
2019-01-31 10:14:12 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!
2019-01-31 10:14:12 INFO  SparkContext:54 - Successfully stopped SparkContext
2019-01-31 10:14:12 INFO  ShutdownHookManager:54 - Shutdown hook called
2019-01-31 10:14:12 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-8e9a53db-a601-48c9-813c-8d8c58ec2135
2019-01-31 11:06:48 INFO  RatingsCounter$:13 - Creating spark context
2019-01-31 11:06:48 WARN  Utils:66 - Your hostname, senthil resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)
2019-01-31 11:06:48 WARN  Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address
2019-01-31 11:06:48 INFO  SparkContext:54 - Running Spark version 2.3.1
2019-01-31 11:06:49 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-01-31 11:06:49 INFO  SparkContext:54 - Submitted application: RatingsCounter
2019-01-31 11:06:49 INFO  SecurityManager:54 - Changing view acls to: senthil
2019-01-31 11:06:49 INFO  SecurityManager:54 - Changing modify acls to: senthil
2019-01-31 11:06:49 INFO  SecurityManager:54 - Changing view acls groups to: 
2019-01-31 11:06:49 INFO  SecurityManager:54 - Changing modify acls groups to: 
2019-01-31 11:06:49 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(senthil); groups with view permissions: Set(); users  with modify permissions: Set(senthil); groups with modify permissions: Set()
2019-01-31 11:06:49 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 35281.
2019-01-31 11:06:50 INFO  SparkEnv:54 - Registering MapOutputTracker
2019-01-31 11:06:50 INFO  SparkEnv:54 - Registering BlockManagerMaster
2019-01-31 11:06:50 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-01-31 11:06:50 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up
2019-01-31 11:06:50 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-e5b463af-0820-4fc9-a34b-b0ff7e5aee21
2019-01-31 11:06:50 INFO  MemoryStore:54 - MemoryStore started with capacity 884.7 MB
2019-01-31 11:06:50 INFO  SparkEnv:54 - Registering OutputCommitCoordinator
2019-01-31 11:06:50 INFO  log:192 - Logging initialized @2828ms
2019-01-31 11:06:50 INFO  Server:346 - jetty-9.3.z-SNAPSHOT
2019-01-31 11:06:50 INFO  Server:414 - Started @2944ms
2019-01-31 11:06:50 INFO  AbstractConnector:278 - Started ServerConnector@73c60324{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-01-31 11:06:50 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.
2019-01-31 11:06:50 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1992eaf4{/jobs,null,AVAILABLE,@Spark}
2019-01-31 11:06:50 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2a3591c5{/jobs/json,null,AVAILABLE,@Spark}
2019-01-31 11:06:50 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@34a75079{/jobs/job,null,AVAILABLE,@Spark}
2019-01-31 11:06:50 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@107ed6fc{/jobs/job/json,null,AVAILABLE,@Spark}
2019-01-31 11:06:50 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1643d68f{/stages,null,AVAILABLE,@Spark}
2019-01-31 11:06:50 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@186978a6{/stages/json,null,AVAILABLE,@Spark}
2019-01-31 11:06:50 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2e029d61{/stages/stage,null,AVAILABLE,@Spark}
2019-01-31 11:06:50 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@132ddbab{/stages/stage/json,null,AVAILABLE,@Spark}
2019-01-31 11:06:50 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@297ea53a{/stages/pool,null,AVAILABLE,@Spark}
2019-01-31 11:06:50 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@acb0951{/stages/pool/json,null,AVAILABLE,@Spark}
2019-01-31 11:06:50 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5bf22f18{/storage,null,AVAILABLE,@Spark}
2019-01-31 11:06:50 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@267f474e{/storage/json,null,AVAILABLE,@Spark}
2019-01-31 11:06:50 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7a7471ce{/storage/rdd,null,AVAILABLE,@Spark}
2019-01-31 11:06:50 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@28276e50{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-01-31 11:06:50 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@62e70ea3{/environment,null,AVAILABLE,@Spark}
2019-01-31 11:06:50 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3efe7086{/environment/json,null,AVAILABLE,@Spark}
2019-01-31 11:06:50 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@675d8c96{/executors,null,AVAILABLE,@Spark}
2019-01-31 11:06:50 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@741b3bc3{/executors/json,null,AVAILABLE,@Spark}
2019-01-31 11:06:50 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2ed3b1f5{/executors/threadDump,null,AVAILABLE,@Spark}
2019-01-31 11:06:50 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@63648ee9{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-01-31 11:06:50 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@68d6972f{/static,null,AVAILABLE,@Spark}
2019-01-31 11:06:50 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@61526469{/,null,AVAILABLE,@Spark}
2019-01-31 11:06:50 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@274872f8{/api,null,AVAILABLE,@Spark}
2019-01-31 11:06:50 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@180e6ac4{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-01-31 11:06:50 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@42b64ab8{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-01-31 11:06:50 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://10.0.2.15:4040
2019-01-31 11:06:50 INFO  Executor:54 - Starting executor ID driver on host localhost
2019-01-31 11:06:50 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39649.
2019-01-31 11:06:50 INFO  NettyBlockTransferService:54 - Server created on 10.0.2.15:39649
2019-01-31 11:06:50 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-01-31 11:06:50 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, 10.0.2.15, 39649, None)
2019-01-31 11:06:50 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 10.0.2.15:39649 with 884.7 MB RAM, BlockManagerId(driver, 10.0.2.15, 39649, None)
2019-01-31 11:06:50 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, 10.0.2.15, 39649, None)
2019-01-31 11:06:50 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 39649, None)
2019-01-31 11:06:51 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@328572f0{/metrics/json,null,AVAILABLE,@Spark}
2019-01-31 11:06:51 INFO  RatingsCounter$:18 - read file...
2019-01-31 11:06:52 INFO  MemoryStore:54 - Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 884.5 MB)
2019-01-31 11:06:52 INFO  MemoryStore:54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 884.5 MB)
2019-01-31 11:06:52 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on 10.0.2.15:39649 (size: 20.4 KB, free: 884.7 MB)
2019-01-31 11:06:52 INFO  SparkContext:54 - Created broadcast 0 from textFile at RatingsCounter.scala:19
2019-01-31 11:06:52 INFO  SparkContext:54 - Invoking stop() from shutdown hook
2019-01-31 11:06:52 INFO  AbstractConnector:318 - Stopped Spark@73c60324{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-01-31 11:06:52 INFO  SparkUI:54 - Stopped Spark web UI at http://10.0.2.15:4040
2019-01-31 11:06:52 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!
2019-01-31 11:06:52 INFO  MemoryStore:54 - MemoryStore cleared
2019-01-31 11:06:52 INFO  BlockManager:54 - BlockManager stopped
2019-01-31 11:06:52 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped
2019-01-31 11:06:52 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!
2019-01-31 11:06:52 INFO  SparkContext:54 - Successfully stopped SparkContext
2019-01-31 11:06:52 INFO  ShutdownHookManager:54 - Shutdown hook called
2019-01-31 11:06:52 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-555a76ad-cf4f-4f55-8626-8ac36dddcf1b
2019-01-31 11:09:58 INFO  RatingsCounter$:13 - Creating spark context
2019-01-31 11:09:59 WARN  Utils:66 - Your hostname, senthil resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)
2019-01-31 11:09:59 WARN  Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address
2019-01-31 11:09:59 INFO  SparkContext:54 - Running Spark version 2.3.1
2019-01-31 11:09:59 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-01-31 11:10:00 INFO  SparkContext:54 - Submitted application: RatingsCounter
2019-01-31 11:10:00 INFO  SecurityManager:54 - Changing view acls to: senthil
2019-01-31 11:10:00 INFO  SecurityManager:54 - Changing modify acls to: senthil
2019-01-31 11:10:00 INFO  SecurityManager:54 - Changing view acls groups to: 
2019-01-31 11:10:00 INFO  SecurityManager:54 - Changing modify acls groups to: 
2019-01-31 11:10:00 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(senthil); groups with view permissions: Set(); users  with modify permissions: Set(senthil); groups with modify permissions: Set()
2019-01-31 11:10:00 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 44319.
2019-01-31 11:10:00 INFO  SparkEnv:54 - Registering MapOutputTracker
2019-01-31 11:10:00 INFO  SparkEnv:54 - Registering BlockManagerMaster
2019-01-31 11:10:00 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-01-31 11:10:00 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up
2019-01-31 11:10:00 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-a7cca40e-9541-42da-9625-a5116c1bd24c
2019-01-31 11:10:00 INFO  MemoryStore:54 - MemoryStore started with capacity 884.7 MB
2019-01-31 11:10:00 INFO  SparkEnv:54 - Registering OutputCommitCoordinator
2019-01-31 11:10:00 INFO  log:192 - Logging initialized @2930ms
2019-01-31 11:10:00 INFO  Server:346 - jetty-9.3.z-SNAPSHOT
2019-01-31 11:10:00 INFO  Server:414 - Started @3092ms
2019-01-31 11:10:01 INFO  AbstractConnector:278 - Started ServerConnector@73c60324{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-01-31 11:10:01 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.
2019-01-31 11:10:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1992eaf4{/jobs,null,AVAILABLE,@Spark}
2019-01-31 11:10:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2a3591c5{/jobs/json,null,AVAILABLE,@Spark}
2019-01-31 11:10:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@34a75079{/jobs/job,null,AVAILABLE,@Spark}
2019-01-31 11:10:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@107ed6fc{/jobs/job/json,null,AVAILABLE,@Spark}
2019-01-31 11:10:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1643d68f{/stages,null,AVAILABLE,@Spark}
2019-01-31 11:10:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@186978a6{/stages/json,null,AVAILABLE,@Spark}
2019-01-31 11:10:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2e029d61{/stages/stage,null,AVAILABLE,@Spark}
2019-01-31 11:10:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@132ddbab{/stages/stage/json,null,AVAILABLE,@Spark}
2019-01-31 11:10:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@297ea53a{/stages/pool,null,AVAILABLE,@Spark}
2019-01-31 11:10:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@acb0951{/stages/pool/json,null,AVAILABLE,@Spark}
2019-01-31 11:10:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5bf22f18{/storage,null,AVAILABLE,@Spark}
2019-01-31 11:10:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@267f474e{/storage/json,null,AVAILABLE,@Spark}
2019-01-31 11:10:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7a7471ce{/storage/rdd,null,AVAILABLE,@Spark}
2019-01-31 11:10:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@28276e50{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-01-31 11:10:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@62e70ea3{/environment,null,AVAILABLE,@Spark}
2019-01-31 11:10:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3efe7086{/environment/json,null,AVAILABLE,@Spark}
2019-01-31 11:10:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@675d8c96{/executors,null,AVAILABLE,@Spark}
2019-01-31 11:10:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@741b3bc3{/executors/json,null,AVAILABLE,@Spark}
2019-01-31 11:10:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2ed3b1f5{/executors/threadDump,null,AVAILABLE,@Spark}
2019-01-31 11:10:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@63648ee9{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-01-31 11:10:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@68d6972f{/static,null,AVAILABLE,@Spark}
2019-01-31 11:10:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@61526469{/,null,AVAILABLE,@Spark}
2019-01-31 11:10:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@274872f8{/api,null,AVAILABLE,@Spark}
2019-01-31 11:10:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@180e6ac4{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-01-31 11:10:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@42b64ab8{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-01-31 11:10:01 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://10.0.2.15:4040
2019-01-31 11:10:01 INFO  Executor:54 - Starting executor ID driver on host localhost
2019-01-31 11:10:01 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36499.
2019-01-31 11:10:01 INFO  NettyBlockTransferService:54 - Server created on 10.0.2.15:36499
2019-01-31 11:10:01 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-01-31 11:10:01 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, 10.0.2.15, 36499, None)
2019-01-31 11:10:01 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 10.0.2.15:36499 with 884.7 MB RAM, BlockManagerId(driver, 10.0.2.15, 36499, None)
2019-01-31 11:10:01 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, 10.0.2.15, 36499, None)
2019-01-31 11:10:01 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 36499, None)
2019-01-31 11:10:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7d2a6eac{/metrics/json,null,AVAILABLE,@Spark}
2019-01-31 11:10:01 INFO  RatingsCounter$:18 - read file...
2019-01-31 11:10:01 INFO  SparkContext:54 - Invoking stop() from shutdown hook
2019-01-31 11:10:01 INFO  AbstractConnector:318 - Stopped Spark@73c60324{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-01-31 11:10:01 INFO  SparkUI:54 - Stopped Spark web UI at http://10.0.2.15:4040
2019-01-31 11:10:01 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!
2019-01-31 11:10:01 INFO  MemoryStore:54 - MemoryStore cleared
2019-01-31 11:10:01 INFO  BlockManager:54 - BlockManager stopped
2019-01-31 11:10:01 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped
2019-01-31 11:10:01 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!
2019-01-31 11:10:01 INFO  SparkContext:54 - Successfully stopped SparkContext
2019-01-31 11:10:01 INFO  ShutdownHookManager:54 - Shutdown hook called
2019-01-31 11:10:01 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-89a3564b-d37e-4269-9efa-e2f66e2549ab
2019-01-31 11:10:57 INFO  RatingsCounter$:13 - Creating spark context
2019-01-31 11:10:57 WARN  Utils:66 - Your hostname, senthil resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)
2019-01-31 11:10:57 WARN  Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address
2019-01-31 11:10:57 INFO  SparkContext:54 - Running Spark version 2.3.1
2019-01-31 11:10:58 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-01-31 11:10:58 INFO  SparkContext:54 - Submitted application: RatingsCounter
2019-01-31 11:10:58 INFO  SecurityManager:54 - Changing view acls to: senthil
2019-01-31 11:10:58 INFO  SecurityManager:54 - Changing modify acls to: senthil
2019-01-31 11:10:58 INFO  SecurityManager:54 - Changing view acls groups to: 
2019-01-31 11:10:58 INFO  SecurityManager:54 - Changing modify acls groups to: 
2019-01-31 11:10:58 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(senthil); groups with view permissions: Set(); users  with modify permissions: Set(senthil); groups with modify permissions: Set()
2019-01-31 11:10:59 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 43677.
2019-01-31 11:10:59 INFO  SparkEnv:54 - Registering MapOutputTracker
2019-01-31 11:10:59 INFO  SparkEnv:54 - Registering BlockManagerMaster
2019-01-31 11:10:59 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-01-31 11:10:59 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up
2019-01-31 11:10:59 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-db2c7fdc-2592-4efc-8b02-7fe1961e6502
2019-01-31 11:10:59 INFO  MemoryStore:54 - MemoryStore started with capacity 884.7 MB
2019-01-31 11:10:59 INFO  SparkEnv:54 - Registering OutputCommitCoordinator
2019-01-31 11:10:59 INFO  log:192 - Logging initialized @2912ms
2019-01-31 11:10:59 INFO  Server:346 - jetty-9.3.z-SNAPSHOT
2019-01-31 11:10:59 INFO  Server:414 - Started @3019ms
2019-01-31 11:10:59 INFO  AbstractConnector:278 - Started ServerConnector@20a7f824{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-01-31 11:10:59 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.
2019-01-31 11:10:59 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7db82169{/jobs,null,AVAILABLE,@Spark}
2019-01-31 11:10:59 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@631e06ab{/jobs/json,null,AVAILABLE,@Spark}
2019-01-31 11:10:59 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2a3591c5{/jobs/job,null,AVAILABLE,@Spark}
2019-01-31 11:10:59 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@346a361{/jobs/job/json,null,AVAILABLE,@Spark}
2019-01-31 11:10:59 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@107ed6fc{/stages,null,AVAILABLE,@Spark}
2019-01-31 11:10:59 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1643d68f{/stages/json,null,AVAILABLE,@Spark}
2019-01-31 11:10:59 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@186978a6{/stages/stage,null,AVAILABLE,@Spark}
2019-01-31 11:10:59 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4052274f{/stages/stage/json,null,AVAILABLE,@Spark}
2019-01-31 11:10:59 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@132ddbab{/stages/pool,null,AVAILABLE,@Spark}
2019-01-31 11:10:59 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@297ea53a{/stages/pool/json,null,AVAILABLE,@Spark}
2019-01-31 11:10:59 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@acb0951{/storage,null,AVAILABLE,@Spark}
2019-01-31 11:10:59 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5bf22f18{/storage/json,null,AVAILABLE,@Spark}
2019-01-31 11:10:59 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@267f474e{/storage/rdd,null,AVAILABLE,@Spark}
2019-01-31 11:10:59 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7a7471ce{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-01-31 11:10:59 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@28276e50{/environment,null,AVAILABLE,@Spark}
2019-01-31 11:10:59 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@62e70ea3{/environment/json,null,AVAILABLE,@Spark}
2019-01-31 11:10:59 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3efe7086{/executors,null,AVAILABLE,@Spark}
2019-01-31 11:10:59 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@675d8c96{/executors/json,null,AVAILABLE,@Spark}
2019-01-31 11:10:59 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@741b3bc3{/executors/threadDump,null,AVAILABLE,@Spark}
2019-01-31 11:10:59 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2ed3b1f5{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-01-31 11:10:59 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@63648ee9{/static,null,AVAILABLE,@Spark}
2019-01-31 11:10:59 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@569bf9eb{/,null,AVAILABLE,@Spark}
2019-01-31 11:10:59 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@61526469{/api,null,AVAILABLE,@Spark}
2019-01-31 11:10:59 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7c351808{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-01-31 11:10:59 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@180e6ac4{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-01-31 11:10:59 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://10.0.2.15:4040
2019-01-31 11:11:00 INFO  Executor:54 - Starting executor ID driver on host localhost
2019-01-31 11:11:00 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40421.
2019-01-31 11:11:00 INFO  NettyBlockTransferService:54 - Server created on 10.0.2.15:40421
2019-01-31 11:11:00 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-01-31 11:11:00 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, 10.0.2.15, 40421, None)
2019-01-31 11:11:00 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 10.0.2.15:40421 with 884.7 MB RAM, BlockManagerId(driver, 10.0.2.15, 40421, None)
2019-01-31 11:11:00 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, 10.0.2.15, 40421, None)
2019-01-31 11:11:00 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 40421, None)
2019-01-31 11:11:00 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@64a1923a{/metrics/json,null,AVAILABLE,@Spark}
2019-01-31 11:11:00 INFO  RatingsCounter$:18 - read file...
2019-01-31 11:11:00 INFO  RatingsCounter$:20 - Path /home/senthil/git/spark_scala/dataAnalysis/target/classes/ml-latest-small/ratings.csv
2019-01-31 11:11:01 INFO  MemoryStore:54 - Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 884.5 MB)
2019-01-31 11:11:01 INFO  MemoryStore:54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 884.5 MB)
2019-01-31 11:11:01 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on 10.0.2.15:40421 (size: 20.4 KB, free: 884.7 MB)
2019-01-31 11:11:01 INFO  SparkContext:54 - Created broadcast 0 from textFile at RatingsCounter.scala:21
2019-01-31 11:11:01 INFO  FileInputFormat:247 - Total input paths to process : 1
2019-01-31 11:11:01 INFO  SparkContext:54 - Starting job: countByValue at RatingsCounter.scala:28
2019-01-31 11:11:01 INFO  DAGScheduler:54 - Registering RDD 4 (countByValue at RatingsCounter.scala:28)
2019-01-31 11:11:01 INFO  DAGScheduler:54 - Got job 0 (countByValue at RatingsCounter.scala:28) with 2 output partitions
2019-01-31 11:11:01 INFO  DAGScheduler:54 - Final stage: ResultStage 1 (countByValue at RatingsCounter.scala:28)
2019-01-31 11:11:01 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 0)
2019-01-31 11:11:01 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 0)
2019-01-31 11:11:01 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at countByValue at RatingsCounter.scala:28), which has no missing parents
2019-01-31 11:11:01 INFO  MemoryStore:54 - Block broadcast_1 stored as values in memory (estimated size 5.1 KB, free 884.5 MB)
2019-01-31 11:11:01 INFO  MemoryStore:54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.9 KB, free 884.5 MB)
2019-01-31 11:11:01 INFO  BlockManagerInfo:54 - Added broadcast_1_piece0 in memory on 10.0.2.15:40421 (size: 2.9 KB, free: 884.7 MB)
2019-01-31 11:11:01 INFO  SparkContext:54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
2019-01-31 11:11:01 INFO  DAGScheduler:54 - Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at countByValue at RatingsCounter.scala:28) (first 15 tasks are for partitions Vector(0, 1))
2019-01-31 11:11:01 INFO  TaskSchedulerImpl:54 - Adding task set 0.0 with 2 tasks
2019-01-31 11:11:01 INFO  TaskSetManager:54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7926 bytes)
2019-01-31 11:11:01 INFO  TaskSetManager:54 - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7926 bytes)
2019-01-31 11:11:01 INFO  Executor:54 - Running task 0.0 in stage 0.0 (TID 0)
2019-01-31 11:11:01 INFO  Executor:54 - Running task 1.0 in stage 0.0 (TID 1)
2019-01-31 11:11:02 INFO  HadoopRDD:54 - Input split: file:/home/senthil/git/spark_scala/dataAnalysis/target/classes/ml-latest-small/ratings.csv:1241861+1241862
2019-01-31 11:11:02 INFO  HadoopRDD:54 - Input split: file:/home/senthil/git/spark_scala/dataAnalysis/target/classes/ml-latest-small/ratings.csv:0+1241861
2019-01-31 11:11:02 INFO  Executor:54 - Finished task 0.0 in stage 0.0 (TID 0). 1194 bytes result sent to driver
2019-01-31 11:11:02 INFO  Executor:54 - Finished task 1.0 in stage 0.0 (TID 1). 1194 bytes result sent to driver
2019-01-31 11:11:02 INFO  TaskSetManager:54 - Finished task 1.0 in stage 0.0 (TID 1) in 704 ms on localhost (executor driver) (1/2)
2019-01-31 11:11:02 INFO  TaskSetManager:54 - Finished task 0.0 in stage 0.0 (TID 0) in 739 ms on localhost (executor driver) (2/2)
2019-01-31 11:11:02 INFO  TaskSchedulerImpl:54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-01-31 11:11:02 INFO  DAGScheduler:54 - ShuffleMapStage 0 (countByValue at RatingsCounter.scala:28) finished in 0.851 s
2019-01-31 11:11:02 INFO  DAGScheduler:54 - looking for newly runnable stages
2019-01-31 11:11:02 INFO  DAGScheduler:54 - running: Set()
2019-01-31 11:11:02 INFO  DAGScheduler:54 - waiting: Set(ResultStage 1)
2019-01-31 11:11:02 INFO  DAGScheduler:54 - failed: Set()
2019-01-31 11:11:02 INFO  DAGScheduler:54 - Submitting ResultStage 1 (ShuffledRDD[5] at countByValue at RatingsCounter.scala:28), which has no missing parents
2019-01-31 11:11:02 INFO  MemoryStore:54 - Block broadcast_2 stored as values in memory (estimated size 3.2 KB, free 884.5 MB)
2019-01-31 11:11:02 INFO  MemoryStore:54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1975.0 B, free 884.5 MB)
2019-01-31 11:11:02 INFO  BlockManagerInfo:54 - Added broadcast_2_piece0 in memory on 10.0.2.15:40421 (size: 1975.0 B, free: 884.7 MB)
2019-01-31 11:11:02 INFO  SparkContext:54 - Created broadcast 2 from broadcast at DAGScheduler.scala:1039
2019-01-31 11:11:02 INFO  DAGScheduler:54 - Submitting 2 missing tasks from ResultStage 1 (ShuffledRDD[5] at countByValue at RatingsCounter.scala:28) (first 15 tasks are for partitions Vector(0, 1))
2019-01-31 11:11:02 INFO  TaskSchedulerImpl:54 - Adding task set 1.0 with 2 tasks
2019-01-31 11:11:02 INFO  TaskSetManager:54 - Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7649 bytes)
2019-01-31 11:11:02 INFO  TaskSetManager:54 - Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 7649 bytes)
2019-01-31 11:11:02 INFO  Executor:54 - Running task 0.0 in stage 1.0 (TID 2)
2019-01-31 11:11:02 INFO  Executor:54 - Running task 1.0 in stage 1.0 (TID 3)
2019-01-31 11:11:02 INFO  ShuffleBlockFetcherIterator:54 - Getting 2 non-empty blocks out of 2 blocks
2019-01-31 11:11:02 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 7 ms
2019-01-31 11:11:02 INFO  ShuffleBlockFetcherIterator:54 - Getting 2 non-empty blocks out of 2 blocks
2019-01-31 11:11:02 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 8 ms
2019-01-31 11:11:02 INFO  Executor:54 - Finished task 1.0 in stage 1.0 (TID 3). 1488 bytes result sent to driver
2019-01-31 11:11:02 INFO  Executor:54 - Finished task 0.0 in stage 1.0 (TID 2). 1407 bytes result sent to driver
2019-01-31 11:11:02 INFO  TaskSetManager:54 - Finished task 0.0 in stage 1.0 (TID 2) in 78 ms on localhost (executor driver) (1/2)
2019-01-31 11:11:02 INFO  TaskSetManager:54 - Finished task 1.0 in stage 1.0 (TID 3) in 83 ms on localhost (executor driver) (2/2)
2019-01-31 11:11:02 INFO  TaskSchedulerImpl:54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-01-31 11:11:02 INFO  DAGScheduler:54 - ResultStage 1 (countByValue at RatingsCounter.scala:28) finished in 0.099 s
2019-01-31 11:11:02 INFO  DAGScheduler:54 - Job 0 finished: countByValue at RatingsCounter.scala:28, took 1.345635 s
2019-01-31 11:11:02 INFO  SparkContext:54 - Invoking stop() from shutdown hook
2019-01-31 11:11:02 INFO  AbstractConnector:318 - Stopped Spark@20a7f824{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-01-31 11:11:02 INFO  SparkUI:54 - Stopped Spark web UI at http://10.0.2.15:4040
2019-01-31 11:11:02 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!
2019-01-31 11:11:02 INFO  MemoryStore:54 - MemoryStore cleared
2019-01-31 11:11:02 INFO  BlockManager:54 - BlockManager stopped
2019-01-31 11:11:02 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped
2019-01-31 11:11:02 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!
2019-01-31 11:11:02 INFO  SparkContext:54 - Successfully stopped SparkContext
2019-01-31 11:11:02 INFO  ShutdownHookManager:54 - Shutdown hook called
2019-01-31 11:11:02 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-d61650bd-e9ae-405e-aac7-c835d646211a
2019-01-31 11:11:28 INFO  RatingsCounter$:13 - Creating spark context
2019-01-31 11:11:28 WARN  Utils:66 - Your hostname, senthil resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)
2019-01-31 11:11:28 WARN  Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address
2019-01-31 11:11:28 INFO  SparkContext:54 - Running Spark version 2.3.1
2019-01-31 11:11:29 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-01-31 11:11:29 INFO  SparkContext:54 - Submitted application: RatingsCounter
2019-01-31 11:11:29 INFO  SecurityManager:54 - Changing view acls to: senthil
2019-01-31 11:11:29 INFO  SecurityManager:54 - Changing modify acls to: senthil
2019-01-31 11:11:29 INFO  SecurityManager:54 - Changing view acls groups to: 
2019-01-31 11:11:29 INFO  SecurityManager:54 - Changing modify acls groups to: 
2019-01-31 11:11:29 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(senthil); groups with view permissions: Set(); users  with modify permissions: Set(senthil); groups with modify permissions: Set()
2019-01-31 11:11:29 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 38811.
2019-01-31 11:11:29 INFO  SparkEnv:54 - Registering MapOutputTracker
2019-01-31 11:11:30 INFO  SparkEnv:54 - Registering BlockManagerMaster
2019-01-31 11:11:30 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-01-31 11:11:30 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up
2019-01-31 11:11:30 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-c4029120-8fb8-4a5a-a79c-0a1963c82127
2019-01-31 11:11:30 INFO  MemoryStore:54 - MemoryStore started with capacity 884.7 MB
2019-01-31 11:11:30 INFO  SparkEnv:54 - Registering OutputCommitCoordinator
2019-01-31 11:11:30 INFO  log:192 - Logging initialized @2778ms
2019-01-31 11:11:30 INFO  Server:346 - jetty-9.3.z-SNAPSHOT
2019-01-31 11:11:30 INFO  Server:414 - Started @2886ms
2019-01-31 11:11:30 INFO  AbstractConnector:278 - Started ServerConnector@709c2f98{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-01-31 11:11:30 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.
2019-01-31 11:11:30 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@f74e835{/jobs,null,AVAILABLE,@Spark}
2019-01-31 11:11:30 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@34a75079{/jobs/json,null,AVAILABLE,@Spark}
2019-01-31 11:11:30 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@346a361{/jobs/job,null,AVAILABLE,@Spark}
2019-01-31 11:11:30 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1643d68f{/jobs/job/json,null,AVAILABLE,@Spark}
2019-01-31 11:11:30 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@186978a6{/stages,null,AVAILABLE,@Spark}
2019-01-31 11:11:30 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2e029d61{/stages/json,null,AVAILABLE,@Spark}
2019-01-31 11:11:30 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@482d776b{/stages/stage,null,AVAILABLE,@Spark}
2019-01-31 11:11:30 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@297ea53a{/stages/stage/json,null,AVAILABLE,@Spark}
2019-01-31 11:11:30 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@acb0951{/stages/pool,null,AVAILABLE,@Spark}
2019-01-31 11:11:30 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5bf22f18{/stages/pool/json,null,AVAILABLE,@Spark}
2019-01-31 11:11:30 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@267f474e{/storage,null,AVAILABLE,@Spark}
2019-01-31 11:11:30 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7a7471ce{/storage/json,null,AVAILABLE,@Spark}
2019-01-31 11:11:30 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@28276e50{/storage/rdd,null,AVAILABLE,@Spark}
2019-01-31 11:11:30 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@62e70ea3{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-01-31 11:11:30 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3efe7086{/environment,null,AVAILABLE,@Spark}
2019-01-31 11:11:30 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@675d8c96{/environment/json,null,AVAILABLE,@Spark}
2019-01-31 11:11:30 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@741b3bc3{/executors,null,AVAILABLE,@Spark}
2019-01-31 11:11:30 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2ed3b1f5{/executors/json,null,AVAILABLE,@Spark}
2019-01-31 11:11:30 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@63648ee9{/executors/threadDump,null,AVAILABLE,@Spark}
2019-01-31 11:11:30 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@68d6972f{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-01-31 11:11:30 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@45be7cd5{/static,null,AVAILABLE,@Spark}
2019-01-31 11:11:30 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@274872f8{/,null,AVAILABLE,@Spark}
2019-01-31 11:11:30 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@76ba13c{/api,null,AVAILABLE,@Spark}
2019-01-31 11:11:30 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@42b64ab8{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-01-31 11:11:30 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7e985ce9{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-01-31 11:11:30 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://10.0.2.15:4040
2019-01-31 11:11:30 INFO  Executor:54 - Starting executor ID driver on host localhost
2019-01-31 11:11:30 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36185.
2019-01-31 11:11:30 INFO  NettyBlockTransferService:54 - Server created on 10.0.2.15:36185
2019-01-31 11:11:30 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-01-31 11:11:30 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, 10.0.2.15, 36185, None)
2019-01-31 11:11:30 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 10.0.2.15:36185 with 884.7 MB RAM, BlockManagerId(driver, 10.0.2.15, 36185, None)
2019-01-31 11:11:30 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, 10.0.2.15, 36185, None)
2019-01-31 11:11:30 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 36185, None)
2019-01-31 11:11:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@678040b3{/metrics/json,null,AVAILABLE,@Spark}
2019-01-31 11:11:31 INFO  RatingsCounter$:18 - read file...
2019-01-31 11:11:31 INFO  MemoryStore:54 - Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 884.5 MB)
2019-01-31 11:11:31 INFO  MemoryStore:54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 884.5 MB)
2019-01-31 11:11:31 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on 10.0.2.15:36185 (size: 20.4 KB, free: 884.7 MB)
2019-01-31 11:11:31 INFO  SparkContext:54 - Created broadcast 0 from textFile at RatingsCounter.scala:21
2019-01-31 11:11:31 INFO  FileInputFormat:247 - Total input paths to process : 1
2019-01-31 11:11:32 INFO  SparkContext:54 - Starting job: countByValue at RatingsCounter.scala:28
2019-01-31 11:11:32 INFO  DAGScheduler:54 - Registering RDD 4 (countByValue at RatingsCounter.scala:28)
2019-01-31 11:11:32 INFO  DAGScheduler:54 - Got job 0 (countByValue at RatingsCounter.scala:28) with 2 output partitions
2019-01-31 11:11:32 INFO  DAGScheduler:54 - Final stage: ResultStage 1 (countByValue at RatingsCounter.scala:28)
2019-01-31 11:11:32 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 0)
2019-01-31 11:11:32 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 0)
2019-01-31 11:11:32 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at countByValue at RatingsCounter.scala:28), which has no missing parents
2019-01-31 11:11:32 INFO  MemoryStore:54 - Block broadcast_1 stored as values in memory (estimated size 5.1 KB, free 884.5 MB)
2019-01-31 11:11:32 INFO  MemoryStore:54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.9 KB, free 884.5 MB)
2019-01-31 11:11:32 INFO  BlockManagerInfo:54 - Added broadcast_1_piece0 in memory on 10.0.2.15:36185 (size: 2.9 KB, free: 884.7 MB)
2019-01-31 11:11:32 INFO  SparkContext:54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
2019-01-31 11:11:32 INFO  DAGScheduler:54 - Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at countByValue at RatingsCounter.scala:28) (first 15 tasks are for partitions Vector(0, 1))
2019-01-31 11:11:32 INFO  TaskSchedulerImpl:54 - Adding task set 0.0 with 2 tasks
2019-01-31 11:11:32 INFO  TaskSetManager:54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7926 bytes)
2019-01-31 11:11:32 INFO  TaskSetManager:54 - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7926 bytes)
2019-01-31 11:11:32 INFO  Executor:54 - Running task 1.0 in stage 0.0 (TID 1)
2019-01-31 11:11:32 INFO  Executor:54 - Running task 0.0 in stage 0.0 (TID 0)
2019-01-31 11:11:32 INFO  HadoopRDD:54 - Input split: file:/home/senthil/git/spark_scala/dataAnalysis/target/classes/ml-latest-small/ratings.csv:0+1241861
2019-01-31 11:11:32 INFO  HadoopRDD:54 - Input split: file:/home/senthil/git/spark_scala/dataAnalysis/target/classes/ml-latest-small/ratings.csv:1241861+1241862
2019-01-31 11:11:33 INFO  Executor:54 - Finished task 1.0 in stage 0.0 (TID 1). 1151 bytes result sent to driver
2019-01-31 11:11:33 INFO  Executor:54 - Finished task 0.0 in stage 0.0 (TID 0). 1151 bytes result sent to driver
2019-01-31 11:11:33 INFO  TaskSetManager:54 - Finished task 1.0 in stage 0.0 (TID 1) in 637 ms on localhost (executor driver) (1/2)
2019-01-31 11:11:33 INFO  TaskSetManager:54 - Finished task 0.0 in stage 0.0 (TID 0) in 663 ms on localhost (executor driver) (2/2)
2019-01-31 11:11:33 INFO  TaskSchedulerImpl:54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-01-31 11:11:33 INFO  DAGScheduler:54 - ShuffleMapStage 0 (countByValue at RatingsCounter.scala:28) finished in 0.772 s
2019-01-31 11:11:33 INFO  DAGScheduler:54 - looking for newly runnable stages
2019-01-31 11:11:33 INFO  DAGScheduler:54 - running: Set()
2019-01-31 11:11:33 INFO  DAGScheduler:54 - waiting: Set(ResultStage 1)
2019-01-31 11:11:33 INFO  DAGScheduler:54 - failed: Set()
2019-01-31 11:11:33 INFO  DAGScheduler:54 - Submitting ResultStage 1 (ShuffledRDD[5] at countByValue at RatingsCounter.scala:28), which has no missing parents
2019-01-31 11:11:33 INFO  MemoryStore:54 - Block broadcast_2 stored as values in memory (estimated size 3.2 KB, free 884.5 MB)
2019-01-31 11:11:33 INFO  MemoryStore:54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1975.0 B, free 884.5 MB)
2019-01-31 11:11:33 INFO  BlockManagerInfo:54 - Added broadcast_2_piece0 in memory on 10.0.2.15:36185 (size: 1975.0 B, free: 884.7 MB)
2019-01-31 11:11:33 INFO  SparkContext:54 - Created broadcast 2 from broadcast at DAGScheduler.scala:1039
2019-01-31 11:11:33 INFO  DAGScheduler:54 - Submitting 2 missing tasks from ResultStage 1 (ShuffledRDD[5] at countByValue at RatingsCounter.scala:28) (first 15 tasks are for partitions Vector(0, 1))
2019-01-31 11:11:33 INFO  TaskSchedulerImpl:54 - Adding task set 1.0 with 2 tasks
2019-01-31 11:11:33 INFO  TaskSetManager:54 - Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7649 bytes)
2019-01-31 11:11:33 INFO  TaskSetManager:54 - Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 7649 bytes)
2019-01-31 11:11:33 INFO  Executor:54 - Running task 1.0 in stage 1.0 (TID 3)
2019-01-31 11:11:33 INFO  Executor:54 - Running task 0.0 in stage 1.0 (TID 2)
2019-01-31 11:11:33 INFO  ShuffleBlockFetcherIterator:54 - Getting 2 non-empty blocks out of 2 blocks
2019-01-31 11:11:33 INFO  ShuffleBlockFetcherIterator:54 - Getting 2 non-empty blocks out of 2 blocks
2019-01-31 11:11:33 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 8 ms
2019-01-31 11:11:33 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 8 ms
2019-01-31 11:11:33 INFO  Executor:54 - Finished task 1.0 in stage 1.0 (TID 3). 1488 bytes result sent to driver
2019-01-31 11:11:33 INFO  Executor:54 - Finished task 0.0 in stage 1.0 (TID 2). 1407 bytes result sent to driver
2019-01-31 11:11:33 INFO  TaskSetManager:54 - Finished task 0.0 in stage 1.0 (TID 2) in 95 ms on localhost (executor driver) (1/2)
2019-01-31 11:11:33 INFO  TaskSetManager:54 - Finished task 1.0 in stage 1.0 (TID 3) in 94 ms on localhost (executor driver) (2/2)
2019-01-31 11:11:33 INFO  TaskSchedulerImpl:54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-01-31 11:11:33 INFO  DAGScheduler:54 - ResultStage 1 (countByValue at RatingsCounter.scala:28) finished in 0.117 s
2019-01-31 11:11:33 INFO  DAGScheduler:54 - Job 0 finished: countByValue at RatingsCounter.scala:28, took 1.250902 s
2019-01-31 11:11:33 INFO  SparkContext:54 - Invoking stop() from shutdown hook
2019-01-31 11:11:33 INFO  BlockManagerInfo:54 - Removed broadcast_1_piece0 on 10.0.2.15:36185 in memory (size: 2.9 KB, free: 884.7 MB)
2019-01-31 11:11:33 INFO  AbstractConnector:318 - Stopped Spark@709c2f98{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-01-31 11:11:33 INFO  SparkUI:54 - Stopped Spark web UI at http://10.0.2.15:4040
2019-01-31 11:11:33 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!
2019-01-31 11:11:33 INFO  MemoryStore:54 - MemoryStore cleared
2019-01-31 11:11:33 INFO  BlockManager:54 - BlockManager stopped
2019-01-31 11:11:33 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped
2019-01-31 11:11:33 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!
2019-01-31 11:11:33 INFO  SparkContext:54 - Successfully stopped SparkContext
2019-01-31 11:11:33 INFO  ShutdownHookManager:54 - Shutdown hook called
2019-01-31 11:11:33 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-68af5130-f6e3-40ec-9f9c-789b7dc71fb9
2019-01-31 11:19:47 INFO  RatingsCounter$:13 - Creating spark context
2019-01-31 11:19:48 WARN  Utils:66 - Your hostname, senthil resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)
2019-01-31 11:19:48 WARN  Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address
2019-01-31 11:19:49 INFO  SparkContext:54 - Running Spark version 2.3.1
2019-01-31 11:19:49 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-01-31 11:19:50 INFO  SparkContext:54 - Submitted application: RatingsCounter
2019-01-31 11:19:50 INFO  SecurityManager:54 - Changing view acls to: senthil
2019-01-31 11:19:50 INFO  SecurityManager:54 - Changing modify acls to: senthil
2019-01-31 11:19:50 INFO  SecurityManager:54 - Changing view acls groups to: 
2019-01-31 11:19:50 INFO  SecurityManager:54 - Changing modify acls groups to: 
2019-01-31 11:19:50 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(senthil); groups with view permissions: Set(); users  with modify permissions: Set(senthil); groups with modify permissions: Set()
2019-01-31 11:19:50 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 33961.
2019-01-31 11:19:50 INFO  SparkEnv:54 - Registering MapOutputTracker
2019-01-31 11:19:50 INFO  SparkEnv:54 - Registering BlockManagerMaster
2019-01-31 11:19:50 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-01-31 11:19:50 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up
2019-01-31 11:19:50 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-816a67a8-652e-4448-9592-4f53c1b421ad
2019-01-31 11:19:50 INFO  MemoryStore:54 - MemoryStore started with capacity 884.7 MB
2019-01-31 11:19:50 INFO  SparkEnv:54 - Registering OutputCommitCoordinator
2019-01-31 11:19:50 INFO  log:192 - Logging initialized @3818ms
2019-01-31 11:19:50 INFO  Server:346 - jetty-9.3.z-SNAPSHOT
2019-01-31 11:19:51 INFO  Server:414 - Started @3953ms
2019-01-31 11:19:51 INFO  AbstractConnector:278 - Started ServerConnector@75de5b60{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-01-31 11:19:51 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.
2019-01-31 11:19:51 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3276732{/jobs,null,AVAILABLE,@Spark}
2019-01-31 11:19:51 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@346a361{/jobs/json,null,AVAILABLE,@Spark}
2019-01-31 11:19:51 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@107ed6fc{/jobs/job,null,AVAILABLE,@Spark}
2019-01-31 11:19:51 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@186978a6{/jobs/job/json,null,AVAILABLE,@Spark}
2019-01-31 11:19:51 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2e029d61{/stages,null,AVAILABLE,@Spark}
2019-01-31 11:19:51 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@482d776b{/stages/json,null,AVAILABLE,@Spark}
2019-01-31 11:19:51 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4052274f{/stages/stage,null,AVAILABLE,@Spark}
2019-01-31 11:19:51 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@acb0951{/stages/stage/json,null,AVAILABLE,@Spark}
2019-01-31 11:19:51 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5bf22f18{/stages/pool,null,AVAILABLE,@Spark}
2019-01-31 11:19:51 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@267f474e{/stages/pool/json,null,AVAILABLE,@Spark}
2019-01-31 11:19:51 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7a7471ce{/storage,null,AVAILABLE,@Spark}
2019-01-31 11:19:51 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@28276e50{/storage/json,null,AVAILABLE,@Spark}
2019-01-31 11:19:51 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@62e70ea3{/storage/rdd,null,AVAILABLE,@Spark}
2019-01-31 11:19:51 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3efe7086{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-01-31 11:19:51 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@675d8c96{/environment,null,AVAILABLE,@Spark}
2019-01-31 11:19:51 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@741b3bc3{/environment/json,null,AVAILABLE,@Spark}
2019-01-31 11:19:51 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2ed3b1f5{/executors,null,AVAILABLE,@Spark}
2019-01-31 11:19:51 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@63648ee9{/executors/json,null,AVAILABLE,@Spark}
2019-01-31 11:19:51 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@68d6972f{/executors/threadDump,null,AVAILABLE,@Spark}
2019-01-31 11:19:51 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@45be7cd5{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-01-31 11:19:51 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7651218e{/static,null,AVAILABLE,@Spark}
2019-01-31 11:19:51 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@76ba13c{/,null,AVAILABLE,@Spark}
2019-01-31 11:19:51 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@eb6449b{/api,null,AVAILABLE,@Spark}
2019-01-31 11:19:51 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7e985ce9{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-01-31 11:19:51 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2a39fe6a{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-01-31 11:19:51 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://10.0.2.15:4040
2019-01-31 11:19:51 INFO  Executor:54 - Starting executor ID driver on host localhost
2019-01-31 11:19:51 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43415.
2019-01-31 11:19:51 INFO  NettyBlockTransferService:54 - Server created on 10.0.2.15:43415
2019-01-31 11:19:51 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-01-31 11:19:51 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, 10.0.2.15, 43415, None)
2019-01-31 11:19:51 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 10.0.2.15:43415 with 884.7 MB RAM, BlockManagerId(driver, 10.0.2.15, 43415, None)
2019-01-31 11:19:51 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, 10.0.2.15, 43415, None)
2019-01-31 11:19:51 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 43415, None)
2019-01-31 11:19:51 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2c0f7678{/metrics/json,null,AVAILABLE,@Spark}
2019-01-31 11:19:51 INFO  RatingsCounter$:18 - read file...
2019-01-31 11:19:52 INFO  MemoryStore:54 - Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 884.5 MB)
2019-01-31 11:19:52 INFO  MemoryStore:54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 884.5 MB)
2019-01-31 11:19:52 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on 10.0.2.15:43415 (size: 20.4 KB, free: 884.7 MB)
2019-01-31 11:19:52 INFO  SparkContext:54 - Created broadcast 0 from textFile at RatingsCounter.scala:20
2019-01-31 11:19:52 INFO  FileInputFormat:247 - Total input paths to process : 1
2019-01-31 11:19:52 INFO  SparkContext:54 - Starting job: countByValue at RatingsCounter.scala:27
2019-01-31 11:19:53 INFO  DAGScheduler:54 - Registering RDD 4 (countByValue at RatingsCounter.scala:27)
2019-01-31 11:19:53 INFO  DAGScheduler:54 - Got job 0 (countByValue at RatingsCounter.scala:27) with 2 output partitions
2019-01-31 11:19:53 INFO  DAGScheduler:54 - Final stage: ResultStage 1 (countByValue at RatingsCounter.scala:27)
2019-01-31 11:19:53 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 0)
2019-01-31 11:19:53 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 0)
2019-01-31 11:19:53 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at countByValue at RatingsCounter.scala:27), which has no missing parents
2019-01-31 11:19:53 INFO  MemoryStore:54 - Block broadcast_1 stored as values in memory (estimated size 5.1 KB, free 884.5 MB)
2019-01-31 11:19:53 INFO  MemoryStore:54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.9 KB, free 884.5 MB)
2019-01-31 11:19:53 INFO  BlockManagerInfo:54 - Added broadcast_1_piece0 in memory on 10.0.2.15:43415 (size: 2.9 KB, free: 884.7 MB)
2019-01-31 11:19:53 INFO  SparkContext:54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
2019-01-31 11:19:53 INFO  DAGScheduler:54 - Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at countByValue at RatingsCounter.scala:27) (first 15 tasks are for partitions Vector(0, 1))
2019-01-31 11:19:53 INFO  TaskSchedulerImpl:54 - Adding task set 0.0 with 2 tasks
2019-01-31 11:19:53 INFO  TaskSetManager:54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7926 bytes)
2019-01-31 11:19:53 INFO  TaskSetManager:54 - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7926 bytes)
2019-01-31 11:19:53 INFO  Executor:54 - Running task 0.0 in stage 0.0 (TID 0)
2019-01-31 11:19:53 INFO  Executor:54 - Running task 1.0 in stage 0.0 (TID 1)
2019-01-31 11:19:53 INFO  HadoopRDD:54 - Input split: file:/home/senthil/git/spark_scala/dataAnalysis/target/classes/ml-latest-small/ratings.csv:0+1241861
2019-01-31 11:19:53 INFO  HadoopRDD:54 - Input split: file:/home/senthil/git/spark_scala/dataAnalysis/target/classes/ml-latest-small/ratings.csv:1241861+1241862
2019-01-31 11:19:54 INFO  Executor:54 - Finished task 0.0 in stage 0.0 (TID 0). 1194 bytes result sent to driver
2019-01-31 11:19:54 INFO  Executor:54 - Finished task 1.0 in stage 0.0 (TID 1). 1194 bytes result sent to driver
2019-01-31 11:19:54 INFO  TaskSetManager:54 - Finished task 0.0 in stage 0.0 (TID 0) in 838 ms on localhost (executor driver) (1/2)
2019-01-31 11:19:54 INFO  TaskSetManager:54 - Finished task 1.0 in stage 0.0 (TID 1) in 826 ms on localhost (executor driver) (2/2)
2019-01-31 11:19:54 INFO  TaskSchedulerImpl:54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-01-31 11:19:54 INFO  DAGScheduler:54 - ShuffleMapStage 0 (countByValue at RatingsCounter.scala:27) finished in 0.963 s
2019-01-31 11:19:54 INFO  DAGScheduler:54 - looking for newly runnable stages
2019-01-31 11:19:54 INFO  DAGScheduler:54 - running: Set()
2019-01-31 11:19:54 INFO  DAGScheduler:54 - waiting: Set(ResultStage 1)
2019-01-31 11:19:54 INFO  DAGScheduler:54 - failed: Set()
2019-01-31 11:19:54 INFO  DAGScheduler:54 - Submitting ResultStage 1 (ShuffledRDD[5] at countByValue at RatingsCounter.scala:27), which has no missing parents
2019-01-31 11:19:54 INFO  MemoryStore:54 - Block broadcast_2 stored as values in memory (estimated size 3.2 KB, free 884.5 MB)
2019-01-31 11:19:54 INFO  MemoryStore:54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1975.0 B, free 884.5 MB)
2019-01-31 11:19:54 INFO  BlockManagerInfo:54 - Added broadcast_2_piece0 in memory on 10.0.2.15:43415 (size: 1975.0 B, free: 884.7 MB)
2019-01-31 11:19:54 INFO  SparkContext:54 - Created broadcast 2 from broadcast at DAGScheduler.scala:1039
2019-01-31 11:19:54 INFO  DAGScheduler:54 - Submitting 2 missing tasks from ResultStage 1 (ShuffledRDD[5] at countByValue at RatingsCounter.scala:27) (first 15 tasks are for partitions Vector(0, 1))
2019-01-31 11:19:54 INFO  TaskSchedulerImpl:54 - Adding task set 1.0 with 2 tasks
2019-01-31 11:19:54 INFO  TaskSetManager:54 - Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7649 bytes)
2019-01-31 11:19:54 INFO  TaskSetManager:54 - Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 7649 bytes)
2019-01-31 11:19:54 INFO  Executor:54 - Running task 0.0 in stage 1.0 (TID 2)
2019-01-31 11:19:54 INFO  Executor:54 - Running task 1.0 in stage 1.0 (TID 3)
2019-01-31 11:19:54 INFO  ShuffleBlockFetcherIterator:54 - Getting 2 non-empty blocks out of 2 blocks
2019-01-31 11:19:54 INFO  ShuffleBlockFetcherIterator:54 - Getting 2 non-empty blocks out of 2 blocks
2019-01-31 11:19:54 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 12 ms
2019-01-31 11:19:54 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 12 ms
2019-01-31 11:19:54 INFO  Executor:54 - Finished task 0.0 in stage 1.0 (TID 2). 1407 bytes result sent to driver
2019-01-31 11:19:54 INFO  TaskSetManager:54 - Finished task 0.0 in stage 1.0 (TID 2) in 97 ms on localhost (executor driver) (1/2)
2019-01-31 11:19:54 INFO  Executor:54 - Finished task 1.0 in stage 1.0 (TID 3). 1488 bytes result sent to driver
2019-01-31 11:19:54 INFO  TaskSetManager:54 - Finished task 1.0 in stage 1.0 (TID 3) in 104 ms on localhost (executor driver) (2/2)
2019-01-31 11:19:54 INFO  TaskSchedulerImpl:54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-01-31 11:19:54 INFO  DAGScheduler:54 - ResultStage 1 (countByValue at RatingsCounter.scala:27) finished in 0.126 s
2019-01-31 11:19:54 INFO  DAGScheduler:54 - Job 0 finished: countByValue at RatingsCounter.scala:27, took 1.513521 s
2019-01-31 11:19:54 INFO  SparkContext:54 - Invoking stop() from shutdown hook
2019-01-31 11:19:54 INFO  AbstractConnector:318 - Stopped Spark@75de5b60{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-01-31 11:19:54 INFO  SparkUI:54 - Stopped Spark web UI at http://10.0.2.15:4040
2019-01-31 11:19:54 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!
2019-01-31 11:19:54 INFO  MemoryStore:54 - MemoryStore cleared
2019-01-31 11:19:54 INFO  BlockManager:54 - BlockManager stopped
2019-01-31 11:19:54 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped
2019-01-31 11:19:54 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!
2019-01-31 11:19:54 INFO  SparkContext:54 - Successfully stopped SparkContext
2019-01-31 11:19:54 INFO  ShutdownHookManager:54 - Shutdown hook called
2019-01-31 11:19:54 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-a5ba7218-cb78-4615-a379-0001f78e5072
2019-01-31 12:04:53 WARN  Utils:66 - Your hostname, senthil resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)
2019-01-31 12:04:53 WARN  Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address
2019-01-31 12:04:53 INFO  SparkContext:54 - Running Spark version 2.3.1
2019-01-31 12:04:53 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-01-31 12:04:54 INFO  SparkContext:54 - Submitted application: Simple Application
2019-01-31 12:04:54 INFO  SecurityManager:54 - Changing view acls to: senthil
2019-01-31 12:04:54 INFO  SecurityManager:54 - Changing modify acls to: senthil
2019-01-31 12:04:54 INFO  SecurityManager:54 - Changing view acls groups to: 
2019-01-31 12:04:54 INFO  SecurityManager:54 - Changing modify acls groups to: 
2019-01-31 12:04:54 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(senthil); groups with view permissions: Set(); users  with modify permissions: Set(senthil); groups with modify permissions: Set()
2019-01-31 12:04:54 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 41367.
2019-01-31 12:04:54 INFO  SparkEnv:54 - Registering MapOutputTracker
2019-01-31 12:04:54 INFO  SparkEnv:54 - Registering BlockManagerMaster
2019-01-31 12:04:54 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-01-31 12:04:54 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up
2019-01-31 12:04:54 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-39739f08-c014-405a-91a3-81475a828a81
2019-01-31 12:04:54 INFO  MemoryStore:54 - MemoryStore started with capacity 884.7 MB
2019-01-31 12:04:54 INFO  SparkEnv:54 - Registering OutputCommitCoordinator
2019-01-31 12:04:54 INFO  log:192 - Logging initialized @2888ms
2019-01-31 12:04:54 INFO  Server:346 - jetty-9.3.z-SNAPSHOT
2019-01-31 12:04:54 INFO  Server:414 - Started @2997ms
2019-01-31 12:04:54 INFO  AbstractConnector:278 - Started ServerConnector@2cac4385{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-01-31 12:04:54 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.
2019-01-31 12:04:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@162be91c{/jobs,null,AVAILABLE,@Spark}
2019-01-31 12:04:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7a48e6e2{/jobs/json,null,AVAILABLE,@Spark}
2019-01-31 12:04:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@b40bb6e{/jobs/job,null,AVAILABLE,@Spark}
2019-01-31 12:04:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5049d8b2{/jobs/job/json,null,AVAILABLE,@Spark}
2019-01-31 12:04:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6d0b5baf{/stages,null,AVAILABLE,@Spark}
2019-01-31 12:04:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@631e06ab{/stages/json,null,AVAILABLE,@Spark}
2019-01-31 12:04:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2a3591c5{/stages/stage,null,AVAILABLE,@Spark}
2019-01-31 12:04:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@107ed6fc{/stages/stage/json,null,AVAILABLE,@Spark}
2019-01-31 12:04:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1643d68f{/stages/pool,null,AVAILABLE,@Spark}
2019-01-31 12:04:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@186978a6{/stages/pool/json,null,AVAILABLE,@Spark}
2019-01-31 12:04:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2e029d61{/storage,null,AVAILABLE,@Spark}
2019-01-31 12:04:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@482d776b{/storage/json,null,AVAILABLE,@Spark}
2019-01-31 12:04:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4052274f{/storage/rdd,null,AVAILABLE,@Spark}
2019-01-31 12:04:55 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@132ddbab{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-01-31 12:04:55 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@297ea53a{/environment,null,AVAILABLE,@Spark}
2019-01-31 12:04:55 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@acb0951{/environment/json,null,AVAILABLE,@Spark}
2019-01-31 12:04:55 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5bf22f18{/executors,null,AVAILABLE,@Spark}
2019-01-31 12:04:55 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@267f474e{/executors/json,null,AVAILABLE,@Spark}
2019-01-31 12:04:55 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7a7471ce{/executors/threadDump,null,AVAILABLE,@Spark}
2019-01-31 12:04:55 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@28276e50{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-01-31 12:04:55 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@62e70ea3{/static,null,AVAILABLE,@Spark}
2019-01-31 12:04:55 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@545de5a4{/,null,AVAILABLE,@Spark}
2019-01-31 12:04:55 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@29ef6856{/api,null,AVAILABLE,@Spark}
2019-01-31 12:04:55 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@569bf9eb{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-01-31 12:04:55 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@61526469{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-01-31 12:04:55 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://10.0.2.15:4040
2019-01-31 12:04:55 INFO  Executor:54 - Starting executor ID driver on host localhost
2019-01-31 12:04:55 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33413.
2019-01-31 12:04:55 INFO  NettyBlockTransferService:54 - Server created on 10.0.2.15:33413
2019-01-31 12:04:55 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-01-31 12:04:55 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, 10.0.2.15, 33413, None)
2019-01-31 12:04:55 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 10.0.2.15:33413 with 884.7 MB RAM, BlockManagerId(driver, 10.0.2.15, 33413, None)
2019-01-31 12:04:55 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, 10.0.2.15, 33413, None)
2019-01-31 12:04:55 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 33413, None)
2019-01-31 12:04:55 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@41c62850{/metrics/json,null,AVAILABLE,@Spark}
2019-01-31 12:04:56 INFO  MemoryStore:54 - Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 884.5 MB)
2019-01-31 12:04:56 INFO  MemoryStore:54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 884.5 MB)
2019-01-31 12:04:56 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on 10.0.2.15:33413 (size: 20.4 KB, free: 884.7 MB)
2019-01-31 12:04:56 INFO  SparkContext:54 - Created broadcast 0 from textFile at SimpleScalaSpark.scala:12
2019-01-31 12:04:56 INFO  SparkContext:54 - Invoking stop() from shutdown hook
2019-01-31 12:04:56 INFO  AbstractConnector:318 - Stopped Spark@2cac4385{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-01-31 12:04:56 INFO  SparkUI:54 - Stopped Spark web UI at http://10.0.2.15:4040
2019-01-31 12:04:56 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!
2019-01-31 12:04:56 INFO  MemoryStore:54 - MemoryStore cleared
2019-01-31 12:04:56 INFO  BlockManager:54 - BlockManager stopped
2019-01-31 12:04:56 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped
2019-01-31 12:04:56 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!
2019-01-31 12:04:56 INFO  SparkContext:54 - Successfully stopped SparkContext
2019-01-31 12:04:56 INFO  ShutdownHookManager:54 - Shutdown hook called
2019-01-31 12:04:56 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-c447a5cf-bb67-456d-9f10-96d9506bc880
2019-01-31 12:05:25 WARN  Utils:66 - Your hostname, senthil resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)
2019-01-31 12:05:25 WARN  Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address
2019-01-31 12:05:26 INFO  SparkContext:54 - Running Spark version 2.3.1
2019-01-31 12:05:26 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-01-31 12:05:26 INFO  SparkContext:54 - Submitted application: Simple Application
2019-01-31 12:05:26 INFO  SecurityManager:54 - Changing view acls to: senthil
2019-01-31 12:05:26 INFO  SecurityManager:54 - Changing modify acls to: senthil
2019-01-31 12:05:27 INFO  SecurityManager:54 - Changing view acls groups to: 
2019-01-31 12:05:27 INFO  SecurityManager:54 - Changing modify acls groups to: 
2019-01-31 12:05:27 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(senthil); groups with view permissions: Set(); users  with modify permissions: Set(senthil); groups with modify permissions: Set()
2019-01-31 12:05:27 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 44189.
2019-01-31 12:05:27 INFO  SparkEnv:54 - Registering MapOutputTracker
2019-01-31 12:05:27 INFO  SparkEnv:54 - Registering BlockManagerMaster
2019-01-31 12:05:27 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-01-31 12:05:27 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up
2019-01-31 12:05:27 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-82d605f6-5d51-488c-a6bc-7fe47b510208
2019-01-31 12:05:27 INFO  MemoryStore:54 - MemoryStore started with capacity 884.7 MB
2019-01-31 12:05:27 INFO  SparkEnv:54 - Registering OutputCommitCoordinator
2019-01-31 12:05:27 INFO  log:192 - Logging initialized @2796ms
2019-01-31 12:05:27 INFO  Server:346 - jetty-9.3.z-SNAPSHOT
2019-01-31 12:05:27 INFO  Server:414 - Started @2897ms
2019-01-31 12:05:27 INFO  AbstractConnector:278 - Started ServerConnector@16f7b4af{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-01-31 12:05:27 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.
2019-01-31 12:05:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1c9f0a20{/jobs,null,AVAILABLE,@Spark}
2019-01-31 12:05:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3a94964{/jobs/json,null,AVAILABLE,@Spark}
2019-01-31 12:05:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5049d8b2{/jobs/job,null,AVAILABLE,@Spark}
2019-01-31 12:05:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@631e06ab{/jobs/job/json,null,AVAILABLE,@Spark}
2019-01-31 12:05:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2a3591c5{/stages,null,AVAILABLE,@Spark}
2019-01-31 12:05:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@34a75079{/stages/json,null,AVAILABLE,@Spark}
2019-01-31 12:05:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@346a361{/stages/stage,null,AVAILABLE,@Spark}
2019-01-31 12:05:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@186978a6{/stages/stage/json,null,AVAILABLE,@Spark}
2019-01-31 12:05:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2e029d61{/stages/pool,null,AVAILABLE,@Spark}
2019-01-31 12:05:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@482d776b{/stages/pool/json,null,AVAILABLE,@Spark}
2019-01-31 12:05:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4052274f{/storage,null,AVAILABLE,@Spark}
2019-01-31 12:05:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@132ddbab{/storage/json,null,AVAILABLE,@Spark}
2019-01-31 12:05:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@297ea53a{/storage/rdd,null,AVAILABLE,@Spark}
2019-01-31 12:05:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@acb0951{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-01-31 12:05:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5bf22f18{/environment,null,AVAILABLE,@Spark}
2019-01-31 12:05:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@267f474e{/environment/json,null,AVAILABLE,@Spark}
2019-01-31 12:05:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7a7471ce{/executors,null,AVAILABLE,@Spark}
2019-01-31 12:05:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@28276e50{/executors/json,null,AVAILABLE,@Spark}
2019-01-31 12:05:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@62e70ea3{/executors/threadDump,null,AVAILABLE,@Spark}
2019-01-31 12:05:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3efe7086{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-01-31 12:05:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@675d8c96{/static,null,AVAILABLE,@Spark}
2019-01-31 12:05:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@ab7a938{/,null,AVAILABLE,@Spark}
2019-01-31 12:05:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3faf2e7d{/api,null,AVAILABLE,@Spark}
2019-01-31 12:05:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@274872f8{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-01-31 12:05:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@76ba13c{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-01-31 12:05:27 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://10.0.2.15:4040
2019-01-31 12:05:28 INFO  Executor:54 - Starting executor ID driver on host localhost
2019-01-31 12:05:28 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38409.
2019-01-31 12:05:28 INFO  NettyBlockTransferService:54 - Server created on 10.0.2.15:38409
2019-01-31 12:05:28 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-01-31 12:05:28 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, 10.0.2.15, 38409, None)
2019-01-31 12:05:28 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 10.0.2.15:38409 with 884.7 MB RAM, BlockManagerId(driver, 10.0.2.15, 38409, None)
2019-01-31 12:05:28 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, 10.0.2.15, 38409, None)
2019-01-31 12:05:28 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 38409, None)
2019-01-31 12:05:28 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@328572f0{/metrics/json,null,AVAILABLE,@Spark}
2019-01-31 12:05:29 INFO  MemoryStore:54 - Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 884.5 MB)
2019-01-31 12:05:29 INFO  MemoryStore:54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 884.5 MB)
2019-01-31 12:05:29 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on 10.0.2.15:38409 (size: 20.4 KB, free: 884.7 MB)
2019-01-31 12:05:29 INFO  SparkContext:54 - Created broadcast 0 from textFile at SimpleScalaSpark.scala:12
2019-01-31 12:05:29 INFO  FileInputFormat:247 - Total input paths to process : 1
2019-01-31 12:05:29 INFO  SparkContext:54 - Starting job: count at SimpleScalaSpark.scala:13
2019-01-31 12:05:29 INFO  DAGScheduler:54 - Got job 0 (count at SimpleScalaSpark.scala:13) with 2 output partitions
2019-01-31 12:05:29 INFO  DAGScheduler:54 - Final stage: ResultStage 0 (count at SimpleScalaSpark.scala:13)
2019-01-31 12:05:29 INFO  DAGScheduler:54 - Parents of final stage: List()
2019-01-31 12:05:29 INFO  DAGScheduler:54 - Missing parents: List()
2019-01-31 12:05:29 INFO  DAGScheduler:54 - Submitting ResultStage 0 (MapPartitionsRDD[2] at filter at SimpleScalaSpark.scala:13), which has no missing parents
2019-01-31 12:05:29 INFO  MemoryStore:54 - Block broadcast_1 stored as values in memory (estimated size 3.4 KB, free 884.5 MB)
2019-01-31 12:05:29 INFO  MemoryStore:54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.0 KB, free 884.5 MB)
2019-01-31 12:05:29 INFO  BlockManagerInfo:54 - Added broadcast_1_piece0 in memory on 10.0.2.15:38409 (size: 2.0 KB, free: 884.7 MB)
2019-01-31 12:05:29 INFO  SparkContext:54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
2019-01-31 12:05:29 INFO  DAGScheduler:54 - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at filter at SimpleScalaSpark.scala:13) (first 15 tasks are for partitions Vector(0, 1))
2019-01-31 12:05:29 INFO  TaskSchedulerImpl:54 - Adding task set 0.0 with 2 tasks
2019-01-31 12:05:29 INFO  TaskSetManager:54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7913 bytes)
2019-01-31 12:05:29 INFO  TaskSetManager:54 - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7913 bytes)
2019-01-31 12:05:29 INFO  Executor:54 - Running task 1.0 in stage 0.0 (TID 1)
2019-01-31 12:05:29 INFO  Executor:54 - Running task 0.0 in stage 0.0 (TID 0)
2019-01-31 12:05:29 INFO  HadoopRDD:54 - Input split: file:/home/senthil/java/apache/spark-2.4.0-bin-hadoop2.7/README.md:0+1976
2019-01-31 12:05:29 INFO  HadoopRDD:54 - Input split: file:/home/senthil/java/apache/spark-2.4.0-bin-hadoop2.7/README.md:1976+1976
2019-01-31 12:05:29 INFO  MemoryStore:54 - Block rdd_1_0 stored as values in memory (estimated size 6.1 KB, free 884.5 MB)
2019-01-31 12:05:29 INFO  BlockManagerInfo:54 - Added rdd_1_0 in memory on 10.0.2.15:38409 (size: 6.1 KB, free: 884.7 MB)
2019-01-31 12:05:29 INFO  MemoryStore:54 - Block rdd_1_1 stored as values in memory (estimated size 5.6 KB, free 884.5 MB)
2019-01-31 12:05:29 INFO  BlockManagerInfo:54 - Added rdd_1_1 in memory on 10.0.2.15:38409 (size: 5.6 KB, free: 884.7 MB)
2019-01-31 12:05:29 INFO  Executor:54 - Finished task 0.0 in stage 0.0 (TID 0). 875 bytes result sent to driver
2019-01-31 12:05:29 INFO  Executor:54 - Finished task 1.0 in stage 0.0 (TID 1). 832 bytes result sent to driver
2019-01-31 12:05:29 INFO  TaskSetManager:54 - Finished task 0.0 in stage 0.0 (TID 0) in 226 ms on localhost (executor driver) (1/2)
2019-01-31 12:05:29 INFO  TaskSetManager:54 - Finished task 1.0 in stage 0.0 (TID 1) in 208 ms on localhost (executor driver) (2/2)
2019-01-31 12:05:29 INFO  TaskSchedulerImpl:54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-01-31 12:05:29 INFO  DAGScheduler:54 - ResultStage 0 (count at SimpleScalaSpark.scala:13) finished in 0.513 s
2019-01-31 12:05:29 INFO  DAGScheduler:54 - Job 0 finished: count at SimpleScalaSpark.scala:13, took 0.584041 s
2019-01-31 12:05:29 INFO  SparkContext:54 - Starting job: count at SimpleScalaSpark.scala:14
2019-01-31 12:05:29 INFO  DAGScheduler:54 - Got job 1 (count at SimpleScalaSpark.scala:14) with 2 output partitions
2019-01-31 12:05:29 INFO  DAGScheduler:54 - Final stage: ResultStage 1 (count at SimpleScalaSpark.scala:14)
2019-01-31 12:05:29 INFO  DAGScheduler:54 - Parents of final stage: List()
2019-01-31 12:05:29 INFO  DAGScheduler:54 - Missing parents: List()
2019-01-31 12:05:29 INFO  DAGScheduler:54 - Submitting ResultStage 1 (MapPartitionsRDD[3] at filter at SimpleScalaSpark.scala:14), which has no missing parents
2019-01-31 12:05:29 INFO  MemoryStore:54 - Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 884.5 MB)
2019-01-31 12:05:29 INFO  MemoryStore:54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.0 KB, free 884.4 MB)
2019-01-31 12:05:29 INFO  BlockManagerInfo:54 - Added broadcast_2_piece0 in memory on 10.0.2.15:38409 (size: 2.0 KB, free: 884.7 MB)
2019-01-31 12:05:29 INFO  SparkContext:54 - Created broadcast 2 from broadcast at DAGScheduler.scala:1039
2019-01-31 12:05:29 INFO  DAGScheduler:54 - Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at filter at SimpleScalaSpark.scala:14) (first 15 tasks are for partitions Vector(0, 1))
2019-01-31 12:05:29 INFO  TaskSchedulerImpl:54 - Adding task set 1.0 with 2 tasks
2019-01-31 12:05:29 INFO  TaskSetManager:54 - Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7913 bytes)
2019-01-31 12:05:29 INFO  TaskSetManager:54 - Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, PROCESS_LOCAL, 7913 bytes)
2019-01-31 12:05:29 INFO  Executor:54 - Running task 0.0 in stage 1.0 (TID 2)
2019-01-31 12:05:29 INFO  Executor:54 - Running task 1.0 in stage 1.0 (TID 3)
2019-01-31 12:05:29 INFO  BlockManager:54 - Found block rdd_1_0 locally
2019-01-31 12:05:29 INFO  BlockManager:54 - Found block rdd_1_1 locally
2019-01-31 12:05:29 INFO  Executor:54 - Finished task 0.0 in stage 1.0 (TID 2). 832 bytes result sent to driver
2019-01-31 12:05:29 INFO  Executor:54 - Finished task 1.0 in stage 1.0 (TID 3). 832 bytes result sent to driver
2019-01-31 12:05:29 INFO  TaskSetManager:54 - Finished task 1.0 in stage 1.0 (TID 3) in 31 ms on localhost (executor driver) (1/2)
2019-01-31 12:05:29 INFO  TaskSetManager:54 - Finished task 0.0 in stage 1.0 (TID 2) in 34 ms on localhost (executor driver) (2/2)
2019-01-31 12:05:29 INFO  TaskSchedulerImpl:54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-01-31 12:05:29 INFO  DAGScheduler:54 - ResultStage 1 (count at SimpleScalaSpark.scala:14) finished in 0.068 s
2019-01-31 12:05:29 INFO  DAGScheduler:54 - Job 1 finished: count at SimpleScalaSpark.scala:14, took 0.076096 s
2019-01-31 12:05:29 INFO  SparkContext:54 - Invoking stop() from shutdown hook
2019-01-31 12:05:29 INFO  AbstractConnector:318 - Stopped Spark@16f7b4af{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-01-31 12:05:29 INFO  SparkUI:54 - Stopped Spark web UI at http://10.0.2.15:4040
2019-01-31 12:05:30 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!
2019-01-31 12:05:30 INFO  MemoryStore:54 - MemoryStore cleared
2019-01-31 12:05:30 INFO  BlockManager:54 - BlockManager stopped
2019-01-31 12:05:30 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped
2019-01-31 12:05:30 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!
2019-01-31 12:05:30 INFO  SparkContext:54 - Successfully stopped SparkContext
2019-01-31 12:05:30 INFO  ShutdownHookManager:54 - Shutdown hook called
2019-01-31 12:05:30 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-0e424954-9ab1-4244-b221-adee37bba1dd
